# ğŸ“˜ Emotion-Aware Brainâ€“Computer Interface (BCI) Using EEG Signals**

## ğŸ§ **Emotion-Aware Brainâ€“Computer Interface for Real-Time Music Control**

**Authors:** Kothariya Mohamad Arman. 
**Dataset:** Prof. Jordan J. Bird â€“ *EEG Brainwave Dataset: Feeling Emotions*

---

# ğŸ§  **1. Project Overview**

This project develops a **real-time, reproducible, and interpretable EEG-based BCI** that can detect emotional state using consumer-grade EEG signals.
The system controls **music playback** based on predicted emotion.

### **Key Achievements**

* **99.06% accuracy** using *Random Forest*
* **Processing latency <10 ms** (true real-time capability)
* **Reproducible ML pipeline** via modular code + config file
* **Statistically validated results (CV, bootstrap, permutation test)**
* **Complete feature-level interpretability (spectral features)**

---

# ğŸ§© **2. Emotion Classes**

We classify EEG signals into **three emotional states**:

* **Positive (P)**
* **Neutral (N)**
* **Negative (N)**

This matches the structure used in Prof. Birdâ€™s dataset.

---

# ğŸ“‚ **3. Repository Structure**

```
BCI-Emotion-Recognition/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ feature_extraction.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ validator.py
â”‚   â”œâ”€â”€ real_time_sim.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_features.ipynb
â”‚   â”œâ”€â”€ 03_training.ipynb
â”‚   â””â”€â”€ 04_validation.ipynb
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ confusion_RF.png
â”‚   â”œâ”€â”€ confusion_XGB.png
â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ latency_breakdown.png
â”‚   â””â”€â”€ accuracy_curve.png
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ System_Architecture.png
â”‚   â”œâ”€â”€ Pipeline.png
â”‚   â””â”€â”€ Mini_Paper.pdf
â”‚
â”œâ”€â”€ config.yaml
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ§ª **4. Dataset Information**

**Dataset Used:** Prof. Jordan J. Bird â€“ *EEG Brainwave Dataset: Feeling Emotions*

* **2131 samples** (4-channel consumer-grade EEG)
* Recorded during emotional stimuli
* Pre-labeled into **Positive, Neutral, Negative**
* Frequency-rich signals ideal for spectral analysis

---

# ğŸ”§ **5. Methodology Pipeline**

### **1. Data Loading**

* Load EEG CSV files
* Merge channels, timestamps, labels
* Clean missing values

### **2. Preprocessing**

* Band-pass filter **1â€“40 Hz**
* Notch filter **50/60 Hz**
* Standardization

### **3. Feature Extraction (Your implementation)**

You selected **Bandpower features**, extracted across canonical EEG bands:

| Band  | Frequency | Emotional Relevance  |
| ----- | --------- | -------------------- |
| Delta | <4 Hz     | Deep cognitive state |
| Theta | 4â€“8 Hz    | Emotional engagement |
| Alpha | 8â€“12 Hz   | Relaxation, calmness |
| Beta  | 12â€“30 Hz  | Arousal, stress      |
| Gamma | >30 Hz    | Higher cognition     |

Calculated for all channels â†’ Feature vector.

### **4. Model Training**

Models tested:

* Random Forest (Best)
* XGBoost
* Logistic Regression (baseline)

---

# ğŸ† **6. Results**

### **Best Model:** **Random Forest**

* **Accuracy:** **99.06%**
* **Latency:** `<10 ms` per inference
* **Balanced performance across all classes**
* Fast & interpretable â†’ ideal for BCI

#### **Confusion Matrix (RF)**

*(Add as image in results folder)*

#### **Feature Importance**

Alpha & Beta bandpower were most influential â†’ matches neuroscience literature.

---

# ğŸ”¬ **7. Validation (Scientific Rigor)**

To confirm results arenâ€™t random or overfitted:

### âœ” **5-Fold Cross Validation**

`Mean = 98.30% Â± 1.01%`

### âœ” **Bootstrap (1000Ã—)**

`95% CI = [98.21%, 99.91%]`

### âœ” **Permutation Test (1000Ã—)**

`p < 0.001`
The model learns meaningful patterns, not noise.

### âœ” **Sanity Check**

Random labels â†’ ~33% accuracy
(Chance level for 3-class problem)

---

# âš¡ **8. Real-Time Mode**

I implemented **real-time simulation**:

* Live feature stream
* Instant classification (ms range)
* Music control logic based on emotion:

  * Positive â†’ Energetic track
  * Neutral â†’ Balanced track
  * Negative â†’ Calming track

This demonstrates **true interactive BCI capability**.

---

# âš ï¸ **9. Limitations**

* Only **4 EEG channels** (limited spatial resolution)
* Neutral vs Negative is still challenging
* Dataset is controlled (not noisy real-world EEG)
* No cross-dataset generalization yet

---

# ğŸš€ **10. Future Work**

* Integrate **OpenBCI/Emotiv** for live streaming
* Add **CSP, entropy, and wavelet features**
* Subject-independent models (transfer learning)
* Validate on larger datasets (DEAP, SEED)
* Expand to IoT controls (lights, appliances)
* Assistive devices (wheelchairs, prosthetics)

---

# ğŸ“¦ **11. Installation**

```
git clone https://github.com/<your-username>/BCI-Emotion-Recognition
cd BCI-Emotion-Recognition
pip install -r requirements.txt
```

Run full pipeline:

```
python run_pipeline.py --config config.yaml
```

---

# ğŸ“ **12. Citation**

Bird, J.J. et al., â€œEEG Brainwave Dataset: Feeling Emotions,â€ **Open Source**, 2020.

---

# ğŸ™Œ **13. Acknowledgments**

* Prof. Jordan Bird (Dataset)
* DEPSTAR IT Department
* Open-source community
